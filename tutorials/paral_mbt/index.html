<!DOCTYPE html>
<html lang="en">
<head>
  
  
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    
    <meta name="author" content="The Abinit group">
    <link rel="canonical" href="http://www.abinit.org/tutorials/paral_mbt/">
    <link rel="shortcut icon" href="../../img/favicon.ico">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" />
    <title>Many-Body - abinit</title>
    <link href="../../css/bootstrap-3.3.7.min.css" rel="stylesheet">
    <link href="../../css/font-awesome-4.7.0.css" rel="stylesheet">
    <link href="../../css/base.css" rel="stylesheet">
    <link rel="stylesheet" href="../../css/highlight.css">
    <link href="../../css/codehilite.css" rel="stylesheet">
    <link href="../../css/extra.css" rel="stylesheet">
    
    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <script src="../../js/jquery-3.2.1.min.js"></script>
    <script src="../../js/bootstrap-3.3.7.min.js"></script>
    <script src="../../js/highlight.pack.js"></script>
    
    <base target="_top">
    <!-- <script src="../../js/somelib.js"></script> -->

    <!-- Jquery from Google CDN 
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
    -->

    <!-- Bootstrap http://getbootstrap.com/getting-started/ -->
    <!-- Latest compiled and minified CSS 

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">
    -->
    <!-- Optional theme

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap-theme.min.css" integrity="sha384-rHyoN1iRsVXV4nD0JutlnGaslCJuC7uwjduW9SVrLvRYooPp2bWYgmgJQIXwl/Sp" crossorigin="anonymous">
    -->
    <!-- Latest compiled and minified JavaScript

    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>
    -->

    <!-- MathJax support -->
    <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
    </script> 
    <!--
    <script type="text/javascript" async 
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_SVG">
    </script> 
    -->

    <!--
    Configure MathJax to produce automatic equation numbers
    http://docs.mathjax.org/en/latest/tex.html
    -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
          TeX: {
              equationNumbers: { autoNumber: "all" },
              extensions: ["AMSmath.js"],
              Macros: {
                  rr: "{\\bf r}",
                  GG: "{\\bf G}",
                  kk: "{\\bf k}",
                  qq: "{\\bf q}"
              }
          }
      });
    </script>

    <!-- See http://docs.mathjax.org/en/latest/start.html 
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        extensions: ["tex2jax.js", "MathMenu.js", "MathZoom.js"],
        jax: ["input/TeX", "output/HTML-CSS", "output/NativeMML],
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true
        },
        "HTML-CSS": { availableFonts: ["TeX"] }
      });
    </script>
    -->


    <script>
      var base_url = '../..';
      var is_top_frame = false;
        
        var pageToc = [
          {title: "1 Generating the WFK file in parallel.", url: "#1-generating-the-wfk-file-in-parallel", children: [
          ]},
          {title: "2. Computing the screening in parallel using the Adler-Wiser expression**", url: "#246-computing-the-screening-in-parallel-using-the-adler-wiser-expression", children: [
              {title: "2.d Manual parallelization over q-points.", url: "#2d-manual-parallelization-over-q-points" },
          ]},
          {title: "3. Computing the screening in parallel using the Hilbert transform", url: "#346-computing-the-screening-in-parallel-using-the-hilbert-transform", children: [
          ]},
          {title: "4. Computing the one-shot GW corrections in parallel**", url: "#446-computing-the-one-shot-gw-corrections-in-parallel", children: [
          ]},
          {title: "5. Basic rules for efficient parallel calculations:", url: "#546-basic-rules-for-efficient-parallel-calculations", children: [
          ]},
        ];

    </script>
    <script src="../../js/base.js"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/ace/1.2.8/ace.js"></script>
      <script src="../../extra_javascript/abidocs.js"></script> 
</head>

<body>
<script>
if (is_top_frame) { $('body').addClass('wm-top-page'); }
</script>



<div class="container-fluid wm-page-content">
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../../theory/theory_mbt/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../../theory/theory_mbt/" class="btn btn-xs btn-link">
        MBPT
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../basepar/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../basepar/" class="btn btn-xs btn-link">
        Base
      </a>
    </div>
    
  </div>

    

    <!-- Return to Top -->

<p><a href="javascript:" id="return-to-top"><i class="glyphicon glyphicon-chevron-up"></i></a>
This lesson aims at showing how to perform parallel calculations with the GW
part of ABINIT. We will discuss the approaches used to parallelize the
different steps of a typical G0W0 calculation, and how to setup the parameters
of the run in order to achieve good speedup. Î±-quartz SiO2 is used as test
case.</p>
<p>It is supposed that you have some knowledge about UNIX/Linux, and you know how
to submit MPI jobs.</p>
<p>This lesson should take about 1.5 hour and requires to have at least a 200 CPU
core parallel computer.</p>
<p>You are supposed to know already some basics of parallelism in ABINIT,
explained in the tutorial <a href="../lesson_basepar.html">A first introduction to ABINIT in
parallel</a>.</p>
<p>In the following, when "run ABINIT over nn CPU cores" appears, you have to use
a specific command line according to the operating system and architecture of
the computer you are using. This can be for instance:</p>
<pre><code>mpirun -n nn abinit &lt; abinit.files
</code></pre>
<p>or the use of a specific submission file.</p>
<ul>
<li>1 Generating the WFK file in parallel. </li>
<li>2 Computing the screening in parallel using the Adler-Wiser expression </li>
<li>3 Computing the screening in parallel using the Hilbert transform method </li>
<li>4 Computing the one-shot GW corrections in parallel </li>
<li>5 Basic rules for efficient parallel calculations </li>
</ul>
<hr />
<p>** The input files necessary to run the examples related to this tutorial are located in the directory ~abinit/tests/tutoparal/Input. </p>
<p>Before beginning, you should create a working directory whose name might be
"Work_mbt" (so ~abinit/tests/tutoparal/Input/Work_mbt).</p>
<p>We will do most of the actions of this tutorial in this working directory.</p>
<h3 id="1-generating-the-wfk-file-in-parallel"><strong>1 Generating the WFK file in parallel.</strong><a class="headerlink" href="#1-generating-the-wfk-file-in-parallel" title="Permanent link">&para;</a></h3>
<p>In the <a href="../lesson_gw1.html">first lesson</a> of the GW tutorial, we have learned how
to generate the WFK file with the sequential version of the code. Now we will
perform a similar calculation taking advantage of the k-point parallelism
implemented in the ground-state part.</p>
<p>First of all, you should copy the files file tmbt_1.files in the working
directory Work_mbt:</p>
<pre><code>$ cd Work_mbt
$ cp ../tmbt_1.files .
</code></pre>
<p>The abinit files files is described in <a href="../../../users/generated_files/help_abinit.html#intro1">section
1.1</a> of the abinit_help
file. Please, read it now if you haven't done it yet!</p>
<p>Now open the input file ~abinit/tests/tutoparal/Input/tmbt_1.in in your
preferred editor, and look at its structure.</p>
<p>The first dataset performs a rather standard SCF calculation to obtain the
ground-state density. The second dataset reads the density file and calculates
the Kohn-Sham band structure including many empty states:</p>
<pre><code># DATASET 2 : WFK generation
iscf2      -2      # NSCF
getden2    -1      # Read previous density
tolwfr2    1d-12   # Stopping criterion for the NSCF cycle.
nband2      160    # Number of (occ and empty) bands computed in the NSCF cycle.
nbdbuf2     10     # A large buffer helps to reduce the number of NSCF steps.
</code></pre>
<p>We have already encountered these variables in the <a href="../lesson_gw1.html">first
lesson</a> of the GW tutorial so their meaning should be
familiar to you.</p>
<p>The only thing worth stressing is that this calculation solves the NSCF cycle
with the conjugate-gradient method (paral_kgb == 0)</p>
<p>The NSCF cycle is executed in parallel using the standard parallelism over
k-points and spin in which the (<a class="wikilink" href="../../input_variables/basic/#nkpt">nkpt</a> x <a class="wikilink" href="../../input_variables/basic/#nsppol">nsppol</a>) blocks of bands are
distributed among the nodes. This test uses an unshifted 4x4x3 grid (48 k
points in the full Brillouin Zone, folding to 9 k-points in the irreducible
wedge) hence the theoretical maximum speedup is 9.</p>
<p>Now run ABINIT over nn CPU cores using</p>
<pre><code>(mpirun ...) abinit &lt; tmbt_1.files &gt;&amp; tmbt_1.log &amp;
</code></pre>
<p>but keep in mind that, to avoid idle processors, the number of CPUs should
divide 9. At the end of the run, the code will produce the file tmbt_1o_WFK
needed for the subsequent GW calculations.</p>
<p>With three nodes, the wall clock time is around 1.5 minutes.</p>
<pre><code>$ tail tmbt_1.out

-
- Proc.   0 individual time (sec): cpu=        209.0  wall=        209.0

================================================================================

 Calculation completed.
.Delivered    0 WARNINGs and   5 COMMENTs to log file.
+Overall time at end (sec) : cpu=        626.9  wall=        626.9
</code></pre>
<p>A reference output file is given in ~tests/tutoparal/Refs, under the name
tmbt_1.out.</p>
<p>Note that 150 bands are not enough to obtain converged GW results, you might
increase the number of bands in proportion to your computing resources.</p>
<hr />
<h3 id="246-computing-the-screening-in-parallel-using-the-adler-wiser-expression">2. Computing the screening in parallel using the Adler-Wiser expression**<a class="headerlink" href="#246-computing-the-screening-in-parallel-using-the-adler-wiser-expression" title="Permanent link">&para;</a></h3>
<p>In this part of the tutorial, we will compute the RPA polarizability with the
Adler-Wiser approach. The basic equations are discussed in this
<a href="../../../theory/generated_files/theorydoc_mbt.html#RPA_Fourier_space">section</a>
of the GW notes.</p>
<p>First copy the files file tmbt_2.file in the working directory, then create a
symbolic link pointing to the WFK file we have generated in the previous step:</p>
<pre><code>$ ln -s tmbt_1o_DS2_WFK tmbt_2i_WFK
</code></pre>
<p>Now open the input file ~abinit/tests/tutoparal/Input/tmbt_2.in so that we can
discuss its structure.</p>
<p>The set of parameters controlling the screening computation is summarized
below:</p>
<pre><code>optdriver   3   # Screening run
irdwfk      1   # Read input WFK file
symchi      1   # Use symmetries to speedup the BZ integration
awtr        1   # Take advantage of time-reversal. Mandatory when gwpara=2 is used.
gwpara      2   # Parallelization over bands
ecutwfn     24  # Cutoff for the wavefunctions.
ecuteps     8   # Cutoff for the polarizability.
nband       50  # Number of bands in the RPA expression (24 occupied bands)
inclvkb     2   # Correct treatment of the optical limit.
</code></pre>
<p>Most of the variables have been already discussed in the
<a href="../lesson_gw1.html">first</a> lesson of the GW tutorial. The only variables that
deserve some additional explanation are <a class="wikilink" href="../../input_variables/paral/#gwpara">gwpara</a> and <a class="wikilink" href="../../input_variables/gw/#awtr">awtr</a>.</p>
<p><a class="wikilink" href="../../input_variables/paral/#gwpara">gwpara</a> selects the parallel algorithm used to compute the screening. Two
different approaches are implemented:</p>
<ul>
<li>
<p><strong>gwpara</strong>=1 -&gt; Trivial parallelization over the k-points in the full Brillouin </p>
</li>
<li>
<p><strong>gwpara</strong>=2 -&gt; Parallelization over bands with memory distribution </p>
</li>
</ul>
<p>Each method presents advantages and drawbacks that are discussed in the
documentation of the variable. In this tutorial, we will be focusing on
<strong>gwpara</strong>=2 since this is the algorithm with the best MPI-scalability and,
mostly important, it is the only one that allows for a significant reduction
of the memory requirement.</p>
<p>The option <a class="wikilink" href="../../input_variables/gw/#awtr">awtr</a>=1 specifies that the system presents time reversal
symmetry so that it is possible to halve the number of transitions that have
to be calculated explicitly (only resonant transitions are needed). Note that
<a class="wikilink" href="../../input_variables/gw/#awtr">awtr</a>=1 is MANDATORY when <a class="wikilink" href="../../input_variables/paral/#gwpara">gwpara</a>=2 is used.</p>
<p>Before running the calculation in parallel, it is worth discussing some
important technical details of the implementation. For our purposes, it
suffices to say that, when <a class="wikilink" href="../../input_variables/paral/#gwpara">gwpara</a>=2 is used in the screening part, the
code distributes the wavefunctions such that each processing unit owns the
FULL set of occupied bands while the empty states are distributed among the
nodes. The parallel computation of the inverse dielectric matrix is done in
three different steps that can be schematically described as follows:</p>
<ol>
<li>Each node computes the partial contribution to the RPA polarizability: </li>
</ol>
<p><img alt="" src="../../documents/lesson_paral_mbt/gwpara2_chi0.png" /></p>
<ol>
<li>
<p>The partial results are collected on each node. </p>
</li>
<li>
<p>The master node performs the matrix inversion to obtain the inverse dielectric matrix and writes the final result on file. </p>
</li>
</ol>
<p>Both the first and second step of the algorithm are expected to scale well
with the number of processors. Step 3, on the contrary, is performed in
sequential thus it will have a detrimental effect on the overall scaling,
especially in the case of large screening matrices (large <a class="wikilink" href="../../input_variables/internal/#npweps">npweps</a> or large
number of frequency points Ï).</p>
<p>Note that the maximum number of CPUs that can be used is dictated by the
number of empty states used to compute the polarizability. Most importantly, a
balanced distribution of the computing time is obtained when the number of
processors divides the number of conduction states.</p>
<p>The main limitation of the present implementation is represented by the
storage of the polarizability. This matrix, indeed, is not distributed hence
each node must have enough memory to store in memory a table whose size is
given by (<strong>npweps</strong>2 x <strong>nomega</strong> x 16 bytes) where <strong>nomega</strong> is the total
number of frequencies computed.</p>
<p>Tests performed at the Barcelona Supercomputing Center (see figures below)
have revealed that the first and the second part of the MPI algorithm have a
very good scaling. The routines cchi0 and cchi0q0 where the RPA expression is
computed (step 1 and 2) scales almost linearly up to 512 processors. The
degradation of the total speedup observed for large number of processors is
mainly due to the portions of the computation that are not parallelized,
namely the reading of the WFK file and the matrix inversion (qloop).</p>
<p><img alt="" src="../../documents/lesson_paral_mbt/screening_speedup.png" /></p>
<p><img alt="" src="../../documents/lesson_paral_mbt/screening_rel_times.png" /></p>
<p>At this point, the most important technical details of the implementation have
been covered, and we can finally run ABINIT over nn CPU cores using</p>
<pre><code>(mpirun ...) abinit &lt; tmbt_2.files &gt;&amp; tmbt_2.log &amp;
</code></pre>
<p>Run the input file tmb_2.in using different number of processors and keep
track of the time for each processor number so that we can test the
scalability of the implementation. The performance analysis reported in the
figures above was obtained with PAW using ZnO as tests case, but you should
observe a similar behavior also in SiO2.</p>
<p>Now let's have a look at the output results. Since this tutorial mainly
focuses on how to run efficient MPI computations, we won't perform any
converge study for SiO2. Most of the parameters used in the input files are
already close to converge, only the k-point sampling and the number of empty
states should be increased. You might modify the input files to perform the
standard converge tests following the procedure described in the <a href="../lesson_gw1.html">first
lesson</a> of the GW tutorial.</p>
<p>In the main output file, there is a section reporting how the bands are
distributed among the nodes. For a sequential calculation, we have</p>
<pre><code> screening : taking advantage of time-reversal symmetry
 Maximum band index for partially occupied states nbvw =    24
 Remaining bands to be divided among processors   nbcw =    26
 Number of bands treated by each node ~   26
</code></pre>
<p>The value reported in the last line will decrease when the computation is done
with more processors.</p>
<p>The memory allocated for the wavefunctions scales with the number of
processors. You can use the grep utility to extract this information from the
log file. For a calculation in sequential, we have:</p>
<pre><code>$ grep "Memory needed" tmbt_2.log

  Memory needed for storing ug=         29.5 [Mb]
  Memory needed for storing ur=        180.2 [Mb]
</code></pre>
<p><em>ug</em> denotes the internal buffer used to store the Fourier components of the
orbitals whose size scales linearly with
<a href="../../../input_variables/generated_files/varint.html#npwwfn">npwwfn</a>. <em>ur</em> is
the array storing the orbitals on the real space FFT mesh. Keep in mind that
the size of <em>ur</em> scales linearly with the total number of points in the FFT
box, number that is usually much larger than the number of planewaves
(<a href="../../../input_variables/generated_files/varint.html#npwwfn">npwwfn</a>). The
number of FFT divisions used in the GW code can be extracted from the main
output file using</p>
<pre><code>$ grep setmesh tmbt_2.out  -A 1
 setmesh: FFT mesh size selected  =  27x 27x 36
          total number of points  =    26244
</code></pre>
<p>As discussed in this
<a href="../../../theory/generated_files/theorydoc_mbt.html#oscillator_notes">section</a> of
the GW notes, the Fast Fourier Transform represents one of the most CPU
intensive part of the execution. For this reason the code provides the input
variable <a class="wikilink" href="../../input_variables/gw/#fftgw">fftgw</a> that can be used to decrease the number of FFT points for
better efficiency. The second digit of the input variable <a class="wikilink" href="../../input_variables/gw/#gwmem">gwmem</a>, instead,
governs the storage of the real space orbitals and can used to avoid the
storage of the costly array <em>ur</em> at the price of an increase in computational
time.</p>
<h4 id="_1"><a class="headerlink" href="#_1" title="Permanent link">&para;</a></h4>
<h4 id="2d-manual-parallelization-over-q-points"><strong> 2.d Manual parallelization over q-points.</strong><a class="headerlink" href="#2d-manual-parallelization-over-q-points" title="Permanent link">&para;</a></h4>
<p>The computational effort required by the screening computation scales linearly
with the number of q-points. As explained in this
<a href="../../../theory/generated_files/theorydoc_mbt.html#RPA_Fourier_space">section</a>
of the GW notes, the code exploits the symmetries of the screening function so
that only the irreducible Brillouin zone (IBZ) has to be calculated
explicitly. On the other hand, a large number of q-points might be needed to
achieve converged results. Typical examples are GW calculations in metals or
optical properties within the Bethe-Salpeter formalism.</p>
<p>If enough processing units are available, the linear factor due to the q-point
sampling can be trivially absorbed by splitting the calculation of the
q-points into several independent runs using the variables <a class="wikilink" href="../../input_variables/gw/#nqptdm">nqptdm</a> and
<a class="wikilink" href="../../input_variables/gw/#qptdm">qptdm</a>. The results can then be gathered in a unique binary file by means
of the <strong>mrgscr</strong> utility (see also the automatic tests v3/t87, v3/t88 and
v3/t89).</p>
<hr />
<h3 id="346-computing-the-screening-in-parallel-using-the-hilbert-transform">3. Computing the screening in parallel using the Hilbert transform<a class="headerlink" href="#346-computing-the-screening-in-parallel-using-the-hilbert-transform" title="Permanent link">&para;</a></h3>
<p>method**</p>
<p>As discussed in the
<a href="../../../theory/generated_files/theorydoc_mbt.html#RPA_Fourier_space">GW_notes</a>,
the algorithm based on the Adler-Wiser expression is not optimal when many
frequencies are wanted. In this paragraph, we therefore discuss how to use the
Hilbert transform method to calculate the RPA polarizability on a dense
frequency mesh. The equations implemented in the code are documented in <a href="../../../theory/generated_files/theorydoc_mbt.html#hilbert_transform">this
section</a> of
the GW notes.</p>
<p>As usual, we have to copy the files file tmbt_3.file in the working directory,
and then create a symbolic link pointing to the WFK file.</p>
<pre><code>$ ln -s tmbt_1o_DS2_WFK tmbt_3i_WFK
</code></pre>
<p>The input file is ~abinit/tests/tutoparal/Input/tmbt_3.in. Open it so that we
can have a look at its structure.</p>
<p>A snapshot of the most important parameters governing the algorithm is
reported below.</p>
<pre><code>gwcalctyp   2    # Contour-deformation technique.
spmeth      1    # Enable the spectral method.
nomegasf  100    # Number of points for the spectral function. 
gwpara      2    # Parallelization over bands
awtr        1    # Take advantage of time-reversal. Mandatory when gwpara=2 is used.
freqremax  40 eV # Frequency mesh for the polarizability
nfreqre    20
nfreqim     5
</code></pre>
<p>The input file is similar to the one we used for the Adler-Wiser calculation.
The input variable <a class="wikilink" href="../../input_variables/gw/#spmeth">spmeth</a> enables the spectral method. <a class="wikilink" href="../../input_variables/gw/#nomegasf">nomegasf</a>
defines the number of Ïâ² points in the linear mesh used for the spectral
function i.e. the number of Ïâ² in the
<a href="../../../theory/generated_files/theorydoc_mbt.html#hilbert_transform">equation</a>
for the spectral function.</p>
<p>As discussed in the <a href="../../../theory/generated_files/theorydoc_mbt.html#hilbert_transform">GW
notes</a>, the
Hilbert transform method is much more memory demanding that the Adler-Wiser
approach, mainly because of the large value of <a class="wikilink" href="../../input_variables/gw/#nomegasf">nomegasf</a> that is usually
needed to converge the results. Fortunately, the particular distribution of
the data employed in <a class="wikilink" href="../../input_variables/paral/#gwpara">gwpara</a>=2 turns out to be well suited for the
calculation of the spectral function since each processor has to store and
treat only a subset of the entire range of transition energies. The algorithm
therefore presents good MPI-scalability since the number of Ïâ² frequencies
that have to be stored and considered in the Hilbert transform decreases with
the number of processors.</p>
<p>Now run ABINIT over nn CPU cores using</p>
<pre><code>(mpirun ...) abinit &lt; tmbt_3.files &gt;&amp; tmbt_3.log &amp;
</code></pre>
<p>and test the scaling by varying the number of processors. Keep in mind that,
also in this case, the distribution of the computing work is well balanced
when the number of CPUs divides the number of conduction states.</p>
<p>The memory needed to store the spectral function is reported in the log file:</p>
<pre><code>$ grep "sf_chi0q0" tmbt_3.log
 memory required by sf_chi0q0:           1.0036 [Gb]
</code></pre>
<p>Note how the size of this array decreases when more processors are used.</p>
<p>The figure below shows the electron energy loss function (EELF) of SiO2
calculated using the Adler-Wiser and the Hilbert transform method. You might
try to reproduce these results (the EELF is reported in the file tmbt_3o_EELF,
a much denser k-sampling is required to achieve convergence).</p>
<p><img alt="" src="../../documents/lesson_paral_mbt/comp-AW-spect.png" /></p>
<hr />
<h3 id="446-computing-the-one-shot-gw-corrections-in-parallel">4. Computing the one-shot GW corrections in parallel**<a class="headerlink" href="#446-computing-the-one-shot-gw-corrections-in-parallel" title="Permanent link">&para;</a></h3>
<p>In this last paragraph, we discuss how to calculate G0W0 corrections in
parallel with <a class="wikilink" href="../../input_variables/paral/#gwpara">gwpara</a>=2. The basic equations used to compute the self-
energy matrix elements are discussed in <a href="../../../theory/generated_files/theorydoc_mbt.html#evaluation_gw_sigma">this
part</a> of
the GW notes.</p>
<p>Before running the calculation, copy the files file tmbt_4.file in the working
directory. Then create two symbolic links for the SCR and the WFK file:</p>
<pre><code>ln -s tmbt_1o_DS2_WFK tmbt_4i_WFK
ln -s tmbt_2o_SCR     tmbt_4i_SCR
</code></pre>
<p>Now open the input file ~abinit/tests/tutoparal/Input/tmbt_4.in.</p>
<p>The most important parameters of the calculation are reported below:</p>
<pre><code>optdriver   4            # Sigma run.
irdwfk      1  
irdscr      1
gwcalctyp   0 ppmodel 1  # G0W0 calculation with the plasmon-pole approximation.
#gwcalctyp  2            # Uncomment this line to use the contour-deformation technique but remember to change the SCR file!
gwpara      2            # Parallelization over bands.
symsigma    1            # To enable the symmetrization of the self-energy matrix elements.
ecutwfn    24            # Cutoff for the wavefunctions.
ecuteps     8            # Cutoff in the correlation part.
ecutsigx   20            # Cutoff in the exchange part.
nband       50           # Number of bands for the correlation part.
</code></pre>
<p>For our purposes, it suffices to say that this input file defines a standard
one-shot calculation with the plasmon-pole model approximation. We refer to
the documentation and to the <a href="../lesson_gw1.html">first lesson</a> of the GW
tutorial for a more complete description of the meaning of these variables.</p>
<p>Also in this case, we use <a class="wikilink" href="../../input_variables/paral/#gwpara">gwpara</a>=2 to perform the calculation in parallel.
Note, however, that the distribution of the orbitals employed in the self-
energy part significantly differs from the one used to compute the screening.
In what follows, we briefly describe the two-step procedure used to distribute
the wavefunctions:</p>
<ol>
<li>
<p>Each node reads and stores in memory the states where the QP corrections are computed (the list of states specified by <a class="wikilink" href="../../input_variables/gw/#kptgw">kptgw</a> and <a class="wikilink" href="../../input_variables/gw/#bdgw">bdgw</a>). </p>
</li>
<li>
<p>The <a class="wikilink" href="../../input_variables/basic/#nband">nband</a> bands are distributed using the following partition scheme: </p>
</li>
</ol>
<p><img alt="" src="../../documents/lesson_paral_mbt/band_distribution_sigma.png" /></p>
<p>where we have assumed a calculation done with four nodes (the index in the box
denotes the MPI rank).</p>
<p>By virtue of the particular distribution adopted, the computation of the
correlation part is expected to scale well with the number CPUs. The maximum
number of processors that can be used is limited by <a class="wikilink" href="../../input_variables/basic/#nband">nband</a>. Note, however,
that only a subset of processors will receive the occupied states when the
bands are distributed in step 2. As a consequence, the theoretical maximum
speedup that can be obtained in the exchange part is limited by the
availability of the occupied states on the different MPI nodes involved in the
run.</p>
<p>The best-case scenario is when the QP corrections are wanted for all the
occupied states. In this case, indeed, each node can compute part of the self-
energy and almost linear scaling should be reached. The worst-case scenario is
when the quasiparticle corrections are wanted only for a few states (e.g. band
gap calculations) and NCPU &gt;&gt; Nvalence. In this case, indeed, only
Nvalence processors will participate to the calculation of the exchange part.</p>
<p>To summarize: The MPI computation of the correlation part is efficient when
the number of processors divides <strong>nband</strong>. Optimal scaling in the exchange
part is obtained only when each node possesses the full set of occupied
states.</p>
<p>The two figures below show the speedup of the sigma part as function of the
number of processors. The self-energy is calculated for 5 quasiparticle states
using nband=1024 (205 occupied states). Note that this setup is close to the
worst-case scenario. The computation of the self-energy matrix elements
(csigme) scales well up to 64 processors. For large number number of CPUs, the
scaling departs from the linear behavior due to the unbalanced distribution of
the occupied bands. The non-scalable parts of the implementation (init1,
rdkss) limit the total speedup due to Amdhal's law.</p>
<p><img alt="" src="../../documents/lesson_paral_mbt/sigma_analysis.png" /></p>
<p>The implementation presents good memory scalability since the largest arrays
are distributed. Only the size of the screening does not scale with the number
of nodes. By default each CPU stores in memory the entire screening matrix for
all the q-points and frequencies in order to optimize the computation. In the
case of large matrices, however, it possible to opt for an out-of-core
solution in which only a single q-point is stored in memory and the data is
read from the external SCR file (slower but less memory demanding). This
option is controlled by the first digit of <a class="wikilink" href="../../input_variables/gw/#gwmem">gwmem</a>.</p>
<p>Now that we know how distribute the load efficiently, we can finally run the
calculation using</p>
<pre><code>(mpirun ...) abinit &lt; tmbt_4.files &gt;&amp; tmbt_4.log &amp;
</code></pre>
<p>Keep track of the time for each processor number so that we can test the
scalability of the self-energy part.</p>
<p>Please note that the results of these tests are not converged. A well
converged calculation would require a 6x6x6 k-mesh to sample the full BZ, and
a cutoff energy of 10 Ha for the screening matrix. The QP results converge
extremely slowly with respect to the number of empty states. To converge the
QP gaps within 0.1 eV accuracy, we had to include 1200 bands in the screening
and 800 states in the calculation of the self-energy.</p>
<p>The comparison between the LDA band structure and the G0W0 energy bands of
Î±-quartz SiO2 is reported in the figure below. The direct gap at Î is opened
up significantly from the LDA value of 6.4 eV to about 9.5 eV when the one-
shot G0W0 method is used. You are invited to reproduce this result (take into
account that this calculation has been performed at the theoretical LDA
parameters, while the experimental structure is used in all the input files of
this tutorial).</p>
<p><img alt="" src="../../documents/lesson_paral_mbt/SiO2_KSG0W0_PPM1.png" /></p>
<hr />
<h3 id="546-basic-rules-for-efficient-parallel-calculations"><strong>5. Basic rules for efficient parallel calculations:</strong><a class="headerlink" href="#546-basic-rules-for-efficient-parallel-calculations" title="Permanent link">&para;</a></h3>
<ol>
<li>
<p>Remember that "Anything that can possibly go wrong, does" so, when writing your input file, try to "Keep It Short and Simple". </p>
</li>
<li>
<p>Do one thing and do it well: <br />
Avoid using different values of <a class="wikilink" href="../../input_variables/gstate/#optdriver">optdriver</a> in the same input file. Each
runlevel employs different approaches to distribute memory and CPU time, hence
it is almost impossible to find the number of processors that will produce a
balanced run in each dataset.</p>
</li>
<li>
<p>Prime number theorem: <br />
Convergence studies should be executed in parallel only when the parameters
that are tested do not interfere with the MPI algorithm. For example, the
convergence study in the number of bands in the screening should be done in
separated input files when <a class="wikilink" href="../../input_variables/paral/#gwpara">gwpara</a>=2 is used.</p>
</li>
<li>
<p>Less is more: <br />
Split big calculations into smaller runs whenever possible. For example,
screening calculations can be split over q-points. The calculation of the
self-energy can be easily split over
<a href="../../../input_variables/generated_files/vargw.html#kptgw">kptgw</a> and
<a href="../../../input_variables/generated_files/vargw.html#bdgw">bdgw</a>.</p>
</li>
<li>
<p>Look before you leap: <br />
Use the converge tests to estimate how the CPU-time and the memory
requirements depend on the parameter that is tested. Having an estimate of the
computing resources is very helpful when one has to launch the final
calculation with converged parameters.</p>
</li>
</ol>

  <br>
    

  <div class="row wm-article-nav-buttons" role="navigation" aria-label="navigation">
    
    <div class="wm-article-nav pull-right">
      <a href="../../theory/theory_mbt/" class="btn btn-xs btn-default pull-right">
        Next
        <i class="fa fa-chevron-right" aria-hidden="true"></i>
      </a>
      <a href="../../theory/theory_mbt/" class="btn btn-xs btn-link">
        MBPT
      </a>
    </div>
    
    <div class="wm-article-nav">
      <a href="../basepar/" class="btn btn-xs btn-default pull-left">
        <i class="fa fa-chevron-left" aria-hidden="true"></i>
        Previous</a><a href="../basepar/" class="btn btn-xs btn-link">
        Base
      </a>
    </div>
    
  </div>

    <br>
</div>

<footer class="col-md-12 wm-page-content">
<hr>

    <center>Abinit version 8.5.4</center>


    <center>Copyright &copy; 2017 The Abinit Group</center>


<center>Documentation built with <a href="http://www.mkdocs.org/">MkDocs</a>.</center>
</footer>

</body>
</html>